---
title: "UWMadison_CS540_Su20_P04"
author: "Hyecheol (Jerry) Jang"
date: "8/18/2020"
output: html_document
---

## Initialization

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)

# Install Required Packages
if(!require("plyr")) {
  install.packages("plyr")
  stopifnot(require("plyr"))
}
if (!require("parallel")) { # for multicore computing functions
  install.packages("parallel")
  stopifnot(require("parallel"))
}
```

```{r initialization}
# Write output file header
output = "Outputs:\n@id\njang52"
# Parallel
n.cores = detectCores()
```


## Data Pre-process

```{r dataPreprocess}
# Read Data
cum_data = read.csv(file = "time_series_covid19_deaths_global.csv")
## Combine multiple rows of same country
cum_data = ddply(cum_data, "Country.Region",numcolwise(sum)) ## ddply in plyr
## Remove unnecessary columns
cum_data = cum_data[, -which(names(cum_data) %in% c("Province.State", "Lat", "Long"))]
## Sort by country name
cum_data = cum_data[(order(cum_data$Country.Region)), ]
rownames(cum_data) = cum_data[,"Country.Region"]

# Generate difference time series
## Function to retrieve difference time series for each row
f_diff_data_row = function(row_index, dataset) {
  ## retrieve 
  original = dataset[row_index, ]
  diff = original
  ## Calculate difference
  for(col_index in 3:length(diff)) {
    diff[col_index] = original[col_index] - original[col_index - 1]
  }
  
  return(diff)
}
## Parallel Operation
if (.Platform$OS.type == "windows") { # on Windows PC
  cluster = makePSOCKcluster(names = n.cores)
  clusterEvalQ(cl = cluster, expr = "") 
  diff_data <- parLapply(cl = cluster, X = 1:length(cum_data[, 1]), fun = f_diff_data_row, dataset = cum_data)
  stopCluster(cl = cluster)
} else { # on Mac or Linux.
  diff_data <- mclapply(X = 1:length(cum_data[, 1]), FUN = f_diff_data_row, dataset = cum_data, mc.cores = n.cores)
}
diff_data <- data.frame(matrix(unlist(diff_data), nrow=length(diff_data), byrow=TRUE))
## Sort by country name
diff_data = diff_data[(order(diff_data[, 1])), ]
rownames(diff_data) = diff_data[, 1]
## Set column name
colnames(diff_data) = colnames(cum_data)
rm(cluster, f_diff_data_row)

# Q1: Enter the cumulative time series for the US and Canada
output = paste(output, "@original",
               paste(as.vector(unname(cum_data["US",2:length(cum_data)])), collapse = ","),
               paste(as.vector(unname(cum_data["Canada",2:length(cum_data)])), collapse = ","),
               sep = "\n")

# Q2: Enter the differenced time series for the US and Canada
output = paste(output, "@difference",
               paste(as.vector(unname(diff_data["US",3:length(diff_data)])), collapse = ","),
               paste(as.vector(unname(diff_data["Canada",3:length(diff_data)])), collapse = ","),
               sep = "\n")
```


## Fit Parametric Model

```{r Q3_explanation}
output = paste(sep = "\n", output, "@answer_3",
"I used optim() function of R in order to find parameters for logistic function: https://en.wikipedia.org/wiki/Logistic_function.
For the loss, I used L1 norm (absolute value of difference between the real datapoint and the predicted point).
While solving this optimiation problem, I used Nelder-Mead method as it does not require derivatives.
Initial value of x_0 is midpoint of given timeframe (104), L's initial value is maximum value of data, and k's initial value is 1.")
```

```{r parametricModel}
# Calculate sum of L1 loss for all data entries in one batch
f_L1 = function(data, par) {
  ## retrieve parameters
  L = par[1]
  k = par[2]
  x_0 = par[3]
  
  ## Retrieve real value and predicted value
  real = data[2:length(data)] ## country name at index 1
  pred = lapply(1:(length(data) - 1), function(x) L / (1 + exp((-1) * k * (x - x_0))))
  ## Calculate L1 loss
  loss = sum(abs(pred - real))
  
  return(loss)
}

# Wrapper function to call optim() to find parameters
f_optim_wrapper = function(arg) {
  data_current = arg$data
  par_current = arg$par
  result = optim(fn = f_L1, data = data_current, par = par_current,
                 method = "Nelder-Mead", ## Not using derivatives
		             control = list(maxit = 1000000))
  return(c(data_current[1], result$convergence,
           round(result$par[1], 2), round(result$par[2], 2), round(result$par[3], 2)))
}

# Arguments
arg_list = list()
for(row_index in 1:length(cum_data[,1])) {
  data = cum_data[row_index,]
  arg = list(data = data, par = c(max(data[2:length(data)]), 1, 104))
  arg_list[[row_index]] = arg
}
rm(data, arg, row_index)

# Run optim() Parallel
if(.Platform$OS.type == "windows") { # on Windows PC
  cluster = makePSOCKcluster(names = n.cores)
  clusterEvalQ(cl = cluster, expr = { # Each Cluster needs to load helper methods
    # Calculate sum of L1 loss for all data entries in one batch
    f_L1 = function(data, par) {
      ## retrieve parameters
      L = par[1]
      k = par[2]
      x_0 = par[3]
      
      ## Retrieve real value and predicted value
      real = data[2:length(data)] ## country name at index 1
      pred = lapply(1:(length(data) - 1), function(x) L / (1 + exp((-1) * k * (x - x_0))))
      ## Calculate L1 loss
      loss = sum(abs(pred - real))
      
      return(loss)
    }
    
    # Wrapper function to call optim() to find parameters
    f_optim_wrapper = function(arg) {
      data_current = arg$data
      par_current = arg$par
      result = optim(fn = f_L1, data = data_current, par = par_current,
                     method = "Nelder-Mead", ## Not using derivatives
    		             control = list(maxit = 1000000))
      return(c(data_current[1], result$convergence,
               round(result$par[1], 2), round(result$par[2], 2), round(result$par[3], 2)))
    }
  })
  optim_result <- parLapply(cl = cluster, X = arg_list, fun = f_optim_wrapper)
  stopCluster(cl = cluster)
} else { # on Mac or Linux.
  optim_result <- mclapply(X = arg_list, FUN = f_optim_wrapper, mc.cores = n.cores)
}
rm(cluster, arg_list, f_L1, f_optim_wrapper)

# Sort by Country Name
optim_result <- data.frame(matrix(unlist(optim_result), nrow=length(optim_result), byrow=TRUE))
optim_result = optim_result[(order(optim_result[, 1])), ]
rownames(optim_result) = optim_result[, 1]
colnames(optim_result) = c("Country.Region", "convergence", "L", "k", "x_0")

# Q4 Input the parameter estimates as a matrix, one row for each country
output = paste(sep = "\n", output, "@parameters")
for(row_index in 1:length(optim_result[, 1])) {
  output = paste(sep = "\n", output,
                 paste(as.vector(unname(optim_result[row_index, 3:5])), collapse = ","))
}
rm(row_index, cum_data, diff_data)
optim_result = optim_result[, -which(names(optim_result) %in% c("convergence"))] # remove unusing column
```


## Hierarchical Clustering

```{r hierarchicalClusteringFunctions}
# Function to calculate single linkage distance
f_single_linkage = function(cluster_1, cluster_2) {
  minDistance = Inf
  
  # Iterate through the cluster
  for(row_index_1 in 1:length(cluster_1[, 1])) {
    for(row_index_2 in 1:length(cluster_2[, 1])) {
      # Extract values
      value_1 = as.numeric(unlist(unname(cluster_1[row_index_1, 2:4])))
      value_2 = as.numeric(unlist(unname(cluster_2[row_index_2, 2:4])))
      
      # calculate new minDistance
      currentDistance = sqrt(sum((value_1 - value_2)^2))
      if(minDistance > currentDistance) {
        minDistance = currentDistance
      }
    }
  }
  
  return(minDistance)
}

# Function to calculate complete linkage distance
f_complete_linkage = function(cluster_1, cluster_2) {
  maxDistance = -Inf
  
  # Iterate through the cluster
  for(row_index_1 in 1:length(cluster_1[, 1])) {
    for(row_index_2 in 1:length(cluster_2[, 1])) {
      # Extract values
      value_1 = as.numeric(unlist(unname(cluster_1[row_index_1, 2:4])))
      value_2 = as.numeric(unlist(unname(cluster_2[row_index_2, 2:4])))
      
      # calculate new maxDistance
      currentDistance = sqrt(sum((value_1 - value_2)^2))
      if(maxDistance < currentDistance) {
        maxDistance = currentDistance
      }
    }
  }
  
  return(maxDistance)
}

f_hierarchical_clustering = function(data, linkage_FUN, k) {
  # Initially, Each entry compose one cluster
  clustering_result = data
  clustering_result = split(clustering_result, seq(nrow(clustering_result)))
  for(cluster_index in 1:length(clustering_result)) {
    clustering_result[[cluster_index]] =
      data.frame(matrix(unlist(clustering_result[[cluster_index]]), nrow = 1, byrow = TRUE))
  }
  
  while(length(clustering_result) > k) {
    # Initialize Search
    minDistance = Inf
    mergeTarget = c(-1, -1)
    
    # Iterate through clusters
    for(cluster_index_1 in 1:length(clustering_result)) {
      cluster_1 = clustering_result[[cluster_index_1]]
      
      for(cluster_index_2 in cluster_index_1:length(clustering_result)) {
        cluster_2 = clustering_result[[cluster_index_2]]
        
        # Only valid when choosing two different cluster
        if(cluster_index_1 != cluster_index_2) {
          # Update Merge Target
          currentDistance = linkage_FUN(cluster_1, cluster_2)
          if(currentDistance < minDistance) { # compute distance
            minDistance = currentDistance
            mergeTarget = c(cluster_index_1, cluster_index_2)
          }
        }
      }
    }
    
    # Merge Two cluster
    clustering_result[[mergeTarget[1]]] = 
      rbind(clustering_result[[mergeTarget[1]]], clustering_result[[mergeTarget[2]]])
    clustering_result[[mergeTarget[[2]]]] = NULL
    
    print(paste("Merge", mergeTarget[1], mergeTarget[2], "|", length(clustering_result), "cluster in the list"))
  }
  
  # Convert result to dataframe
  for(cluster_index in 1:length(clustering_result)) { # Add new column indicating cluster
    clustering_result[[cluster_index]]$cluster =
      rep(cluster_index - 1, times = length(clustering_result[[cluster_index]][,1]))
  }
  final = clustering_result[[1]] # combine dataframes into one dataframe
  for(cluster_index in 2:length(clustering_result)) {
    final = rbind(final, clustering_result[[cluster_index]])
  }
  # sort
  final = final[order(final$X1), ]
  
  return(final)
}
```

```{r singleLinkage}
# Get clustering Result
hacs = f_hierarchical_clustering(optim_result, f_single_linkage, 6)

# Q5: Input the clusters from single linkage hierarchical clustering
output = paste(sep = "\n",
               output, "@hacs",
               paste(as.vector(unname(hacs$cluster)), collapse = ","))
rm(hacs)
```

```{r hacc}
# Get clustering Result
hacc = f_hierarchical_clustering(optim_result, f_complete_linkage, 6)

# Q6: Input the clusters from single linkage hierarchical clustering
output = paste(sep = "\n",
               output, "@hacc",
               paste(as.vector(unname(hacc$cluster)), collapse = ","))

rm(hacc, f_complete_linkage, f_hierarchical_clustering, f_single_linkage)
```


## K-means clustering

```{r kMeansClusteringFunction}
f_k_means = function(data, k) {
  # Random Initialization of centers
  center = list() ## list to store centers
  center_index = sample(1:length(data[, 1]), size = k)
  for(current_index in center_index) {
    center[[length(center) + 1]] = data[current_index, 2:4]
  }
  data$cluster = rep(-1, times = length(data[, 1]))
  
  # Find Initial cluster
  for(data_index in 1:length(data[, 1])) {
    minDistance = Inf
    minCluster = -1
    value_1 = as.numeric(unlist(unname(data[data_index, 2:4])))
    
    for(center_index in 1:k) {
      value_2 = as.numeric(unlist(unname(center[[center_index]])))
      distance = sqrt(sum((value_1 - value_2)^2))
      
      # Find the best cluster
      if(distance < minDistance) {
        minCluster = center_index
        minDistance = distance
      }
    }
    
    # Assign cluster
    data[data_index, "cluster"] = minCluster
  }
  
  # Update until clusters do not change
  keepUpdate = TRUE
  
  while(keepUpdate) {
    # Update center
    prevCenter = center
    for(center_index in 1:k) {
      data_center = data[which(data$cluster == center_index), ]
      center[[center_index]]$"L" = round(mean(as.numeric(data_center$"L")), 4)
      center[[center_index]]$"k" = round(mean(as.numeric(data_center$"k")), 4)
      center[[center_index]]$"x_0" = round(mean(as.numeric(data_center$"x_0")), 4)
    }
    
    # Check for termination
    checker = 0
    for(center_index in 1:k) {
      if(all(center[[center_index]] == prevCenter[[center_index]])) {
        checker = checker + 1
      }
    }
    print(paste("checker:", checker))
    if(checker == k) {
      keepUpdate = FALSE
      break
    }
    
    # update cluster
    for(data_index in 1:length(data[, 1])) {
      minDistance = Inf
      minCluster = -1
      value_1 = as.numeric(unlist(unname(data[data_index, 2:4])))
      
      for(center_index in 1:k) {
        value_2 = as.numeric(unlist(unname(center[[center_index]])))
        distance = sqrt(sum((value_1 - value_2)^2))
        
        # Find the best cluster
        if(distance < minDistance) {
          minCluster = center_index
          minDistance = distance
        }
      }
      
      # Assign cluster
      data[data_index, "cluster"] = minCluster
    }
  }
  
  return(list(data = data, center = center))
}
```

```{r kMeansClustering}
k_mean_result = f_k_means(optim_result, 6)

# Q7: Input the clusters from k means clustering
output = paste(sep = "\n", output, "@kmeans",
               paste(as.vector(as.numeric(unname(k_mean_result[["data"]][, "cluster"])) - 1), collapse = ","))
```


## Post Operations

```{r cleanup}
output = paste(sep = "\n", output, "@answer_10", "None") # Footer for output file
write(output, file = "output.txt", append = FALSE) # write results
rm(list = ls())
```


