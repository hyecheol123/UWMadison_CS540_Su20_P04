---
title: "UWMadison_CS540_Su20_P04"
author: "Hyecheol (Jerry) Jang"
date: "8/18/2020"
output: html_document
---

## Initialization

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)

# Install Required Packages
if(!require("plyr")) {
  install.packages("plyr")
  stopifnot(require("plyr"))
}
if (!require("parallel")) { # for multicore computing functions
  install.packages("parallel")
  stopifnot(require("parallel"))
}
```

```{r initialization}
# Write output file header
output = "Outputs:\n@id\njang52"
# Parallel
n.cores = detectCores()
```


## Data Pre-process

```{r dataPreprocess}
# Read Data
cum_data = read.csv(file = "time_series_covid19_deaths_global.csv")
## Combine multiple rows of same country
cum_data = ddply(cum_data, "Country.Region",numcolwise(sum)) ## ddply in plyr
## Remove unnecessary columns
cum_data = cum_data[, -which(names(cum_data) %in% c("Province.State", "Lat", "Long"))]
## Sort by country name
cum_data = cum_data[(order(cum_data$Country.Region)), ]
rownames(cum_data) = cum_data[,"Country.Region"]

# Generate difference time series
## Function to retrieve difference time series for each row
f_diff_data_row = function(row_index, dataset) {
  ## retrieve 
  original = dataset[row_index, ]
  diff = original
  ## Calculate difference
  for(col_index in 3:length(diff)) {
    diff[col_index] = original[col_index] - original[col_index - 1]
  }
  
  return(diff)
}
## Parallel Operation
if (.Platform$OS.type == "windows") { # on Windows PC
  cluster = makePSOCKcluster(names = n.cores)
  clusterEvalQ(cl = cluster, expr = "") 
  diff_data <- parLapply(cl = cluster, X = 1:length(cum_data[, 1]), fun = f_diff_data_row, dataset = cum_data)
  stopCluster(cl = cluster)
} else { # on Mac or Linux.
  diff_data <- mclapply(X = 1:length(cum_data[, 1]), FUN = f_diff_data_row, dataset = cum_data, mc.cores = n.cores)
}
diff_data <- data.frame(matrix(unlist(diff_data), nrow=length(diff_data), byrow=TRUE))
## Sort by country name
diff_data = diff_data[(order(diff_data[, 1])), ]
rownames(diff_data) = diff_data[, 1]
## Set column name
colnames(diff_data) = colnames(cum_data)
rm(cluster, f_diff_data_row)

# Q1: Enter the cumulative time series for the US and Canada
output = paste(output, "@original",
               paste(as.vector(unname(cum_data["US",2:length(cum_data)])), collapse = ","),
               paste(as.vector(unname(cum_data["Canada",2:length(cum_data)])), collapse = ","),
               sep = "\n")

# Q2: Enter the differenced time series for the US and Canada
output = paste(output, "@difference",
               paste(as.vector(unname(diff_data["US",3:length(diff_data)])), collapse = ","),
               paste(as.vector(unname(diff_data["Canada",3:length(diff_data)])), collapse = ","),
               sep = "\n")
```


## Fit Parametric Model

```{r Q3_explanation}
output = paste(sep = "\n", output, "@answer_3",
"I used optim() function of R in order to find parameters for logistic function: https://en.wikipedia.org/wiki/Logistic_function.
For the loss, I used L1 norm (absolute value of difference between the real datapoint and the predicted point).
While solving this optimiation problem, I used Nelder-Mead method as it does not require derivatives.
Initial value of x_0 is midpoint of given timeframe (104), L's initial value is maximum value of data, and k's initial value is 1.")
```

```{r parametricModel}

```


## Post Operations

```{r cleanup}
output = paste(sep = "\n", output, "@answer_10", "None") # Footer for output file
write(output, file = "output.txt", append = FALSE) # write results
rm(list = ls())
```


