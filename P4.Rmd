---
title: "UWMadison_CS540_Su20_P04"
author: "Hyecheol (Jerry) Jang"
date: "8/18/2020"
output: html_document
---

## Initialization

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)

# Install Required Packages
if(!require("plyr")) {
  install.packages("plyr")
  stopifnot(require("plyr"))
}
if (!require("parallel")) { # for multicore computing functions
  install.packages("parallel")
  stopifnot(require("parallel"))
}
```

```{r initialization}
# Write output file header
output = "Outputs:\n@id\njang52"
# Parallel
n.cores = detectCores()
```


## Data Pre-process

```{r dataPreprocess}
# Read Data
cum_data = read.csv(file = "time_series_covid19_deaths_global.csv")
## Combine multiple rows of same country
cum_data = ddply(cum_data, "Country.Region",numcolwise(sum)) ## ddply in plyr
## Remove unnecessary columns
cum_data = cum_data[, -which(names(cum_data) %in% c("Province.State", "Lat", "Long"))]
## Sort by country name
cum_data = cum_data[(order(cum_data$Country.Region)), ]
rownames(cum_data) = cum_data[,"Country.Region"]

# Generate difference time series
## Function to retrieve difference time series for each row
f_diff_data_row = function(row_index, dataset) {
  ## retrieve 
  original = dataset[row_index, ]
  diff = original
  ## Calculate difference
  for(col_index in 3:length(diff)) {
    diff[col_index] = original[col_index] - original[col_index - 1]
  }
  
  return(diff)
}
## Parallel Operation
if (.Platform$OS.type == "windows") { # on Windows PC
  cluster = makePSOCKcluster(names = n.cores)
  clusterEvalQ(cl = cluster, expr = "") 
  diff_data <- parLapply(cl = cluster, X = 1:length(cum_data[, 1]), fun = f_diff_data_row, dataset = cum_data)
  stopCluster(cl = cluster)
} else { # on Mac or Linux.
  diff_data <- mclapply(X = 1:length(cum_data[, 1]), FUN = f_diff_data_row, dataset = cum_data, mc.cores = n.cores)
}
diff_data <- data.frame(matrix(unlist(diff_data), nrow=length(diff_data), byrow=TRUE))
## Sort by country name
diff_data = diff_data[(order(diff_data[, 1])), ]
rownames(diff_data) = diff_data[, 1]
## Set column name
colnames(diff_data) = colnames(cum_data)
rm(cluster, f_diff_data_row)

# Q1: Enter the cumulative time series for the US and Canada
output = paste(output, "@original",
               paste(as.vector(unname(cum_data["US",2:length(cum_data)])), collapse = ","),
               paste(as.vector(unname(cum_data["Canada",2:length(cum_data)])), collapse = ","),
               sep = "\n")

# Q2: Enter the differenced time series for the US and Canada
output = paste(output, "@difference",
               paste(as.vector(unname(diff_data["US",3:length(diff_data)])), collapse = ","),
               paste(as.vector(unname(diff_data["Canada",3:length(diff_data)])), collapse = ","),
               sep = "\n")
```


## Fit Parametric Model

```{r Q3_explanation}
output = paste(sep = "\n", output, "@answer_3",
"I used optim() function of R in order to find parameters for logistic function: https://en.wikipedia.org/wiki/Logistic_function.
For the loss, I used L1 norm (absolute value of difference between the real datapoint and the predicted point).
While solving this optimiation problem, I used Nelder-Mead method as it does not require derivatives.
Initial value of x_0 is midpoint of given timeframe (104), L's initial value is maximum value of data, and k's initial value is 1.")
```

```{r parametricModel}
# Calculate sum of L1 loss for all data entries in one batch
f_L1 = function(data, par) {
  ## retrieve parameters
  L = par[1]
  k = par[2]
  x_0 = par[3]
  
  ## Retrieve real value and predicted value
  real = data[2:length(data)] ## country name at index 1
  pred = lapply(1:(length(data) - 1), function(x) L / (1 + exp((-1) * k * (x - x_0))))
  ## Calculate L1 loss
  loss = sum(abs(pred - real))
  
  return(loss)
}

# Wrapper function to call optim() to find parameters
f_optim_wrapper = function(arg) {
  data_current = arg$data
  par_current = arg$par
  result = optim(fn = f_L1, data = data_current, par = par_current,
                 method = "Nelder-Mead", ## Not using derivatives
		             control = list(maxit = 1000000))
  return(c(data_current[1], result$convergence,
           round(result$par[1], 2), round(result$par[2], 2), round(result$par[3], 2)))
}

# Arguments
arg_list = list()
for(row_index in 1:length(cum_data[,1])) {
  data = cum_data[row_index,]
  arg = list(data = data, par = c(max(data[2:length(data)]), 1, 104))
  arg_list[[row_index]] = arg
}
rm(data, arg, row_index)

# Run optim() Parallel
if(.Platform$OS.type == "windows") { # on Windows PC
  cluster = makePSOCKcluster(names = n.cores)
  clusterEvalQ(cl = cluster, expr = { # Each Cluster needs to load helper methods
    # Calculate sum of L1 loss for all data entries in one batch
    f_L1 = function(data, par) {
      ## retrieve parameters
      L = par[1]
      k = par[2]
      x_0 = par[3]
      
      ## Retrieve real value and predicted value
      real = data[2:length(data)] ## country name at index 1
      pred = lapply(1:(length(data) - 1), function(x) L / (1 + exp((-1) * k * (x - x_0))))
      ## Calculate L1 loss
      loss = sum(abs(pred - real))
      
      return(loss)
    }
    
    # Wrapper function to call optim() to find parameters
    f_optim_wrapper = function(arg) {
      data_current = arg$data
      par_current = arg$par
      result = optim(fn = f_L1, data = data_current, par = par_current,
                     method = "Nelder-Mead", ## Not using derivatives
    		             control = list(maxit = 1000000))
      return(c(data_current[1], result$convergence,
               round(result$par[1], 2), round(result$par[2], 2), round(result$par[3], 2)))
    }
  })
  optim_result <- parLapply(cl = cluster, X = arg_list, fun = f_optim_wrapper)
  stopCluster(cl = cluster)
} else { # on Mac or Linux.
  optim_result <- mclapply(X = arg_list, FUN = f_optim_wrapper, mc.cores = n.cores)
}
rm(cluster, arg_list, f_L1, f_optim_wrapper)

# Sort by Country Name
optim_result <- data.frame(matrix(unlist(optim_result), nrow=length(optim_result), byrow=TRUE))
optim_result = optim_result[(order(optim_result[, 1])), ]
rownames(optim_result) = optim_result[, 1]
colnames(optim_result) = c("Country.Region", "convergence", "L", "k", "x_0")

# Q4 Input the parameter estimates as a matrix, one row for each country
output = paste(sep = "\n", output, "@parameters")
for(row_index in 1:length(optim_result[, 1])) {
  output = paste(sep = "\n", output,
                 paste(as.vector(unname(optim_result[row_index, 3:5])), collapse = ","))
}
rm(row_index, cum_data, diff_data)
```


## Hierarchical Clustering

```{r}

```


## Post Operations

```{r cleanup}
output = paste(sep = "\n", output, "@answer_10", "None") # Footer for output file
write(output, file = "output.txt", append = FALSE) # write results
rm(list = ls())
```


